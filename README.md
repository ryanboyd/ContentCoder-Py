# ContentCoder

![AI Reading Machine](images/DALL¬∑E%202023-02-01%2016.58.23%20-%20a%20machine%20that%20reads%20books%20and%20has%20good%20ideas,%20high%20quality%203d%20digital%20art.png)


ContentCoder is a Python-based text analysis tool that enables users to process and analyze text using custom linguistic dictionaries. It is inspired by tools like **LIWC (Linguistic Inquiry and Word Count)** and provides robust methods for tokenization, text analysis, and frequency calculations. As noted in a much older version of the README.MD, this is a stripped-down, feature-incomplete version of several tools used in past projects.

Note that like 98% of this readme was generated by ChatGPT ‚Äî it may not be entirely accurate, but at a quick glance, it looks pretty spot-on üòÖü§û

## üî• Features

- **Custom Dictionary-Based Analysis**  
- **Support for LIWC-style dictionaries (2007 & 2022 formats)**  
- **Efficient text tokenization**  
- **Wildcard and abbreviation handling**  
- **Punctuation and big word analysis**  
- **Dictionary export in multiple formats (JSON, CSV, Poster format, etc.)**  
- **High-performance wildcard matching with memory optimization**

---

## üöÄ Installation

Make sure you have Python 3.9+ installed. Clone this repository and install dependencies:

```bash
pip install contentcoder
```

---

## üìÅ Folder Structure

```
src/contentcoder/
‚îÇ‚îÄ‚îÄ __init__.py
‚îÇ‚îÄ‚îÄ ContentCoder.py
‚îÇ‚îÄ‚îÄ ContentCodingDictionary.py
‚îÇ‚îÄ‚îÄ happiestfuntokenizing.py
‚îÇ‚îÄ‚îÄ create_export_dir.py
```

---

## üìå Quick Start

### **1. Import the `ContentCoder` class**
```python
from contentcoder.ContentCoder import ContentCoder
```

### **2. Initialize the Analyzer**
```python
cc = ContentCoder(dicFilename='path/to/dictionary.dic', fileEncoding='utf-8-sig')
```

### **3. Analyze a Text Sample**
```python
text = "Libraries are crucial to our society."
results = cc.Analyze(text, relativeFreq=True, dropPunct=True, retainCaptures=True, returnTokens=False, wildcardMem=True)
print(results)
```

Expected output:
```json
{
  "WC": 6,
  "Dic": 4.5,
  "BigWords": 2.0,
  "Numbers": 0.0,
  "AllPunct": 0.0,
  "Period": 0.0,
  "Comma": 0.0,
  "QMark": 0.0,
  "Exclam": 0.0,
  "Apostro": 0.0,
  "Libraries": 1.0,
  "crucial": 1.0,
  "society": 1.0
}
```

---

## üìñ **Main Functions & Usage**

### **1Ô∏è‚É£ `Analyze(text, **options)`**
Analyzes a given text and returns a dictionary of results.

#### **Parameters:**
- `inputText` _(str)_: The text to analyze.
- `relativeFreq` _(bool)_: If `True`, returns relative frequencies. Otherwise, raw frequencies.
- `dropPunct` _(bool)_: If `True`, punctuation is removed before processing.
- `retainCaptures` _(bool)_: If `True`, captures and stores wildcard-matched words.
- `returnTokens` _(bool)_: If `True`, returns tokenized text.
- `wildcardMem` _(bool)_: If `True`, speeds up wildcard processing by storing past matches.

#### **Example Usage:**
```python
result = cc.Analyze("Hello world! This is a test sentence.", returnTokens=relativeFreq=True)
```

---

### **2Ô∏è‚É£ `GetResultsHeader()`**
Returns a list of all available output categories.

#### **Example Usage:**
```python
print(cc.GetResultsHeader())
```

Expected output:
```json
["WC", "Dic", "BigWords", "Numbers", "AllPunct", "Period", "Comma", "QMark", "Exclam", "Apostro"]
```

---

### **3Ô∏è‚É£ `GetResultsArray(resultsDICT, rounding=4)`**
Formats the results of `Analyze()` into a CSV-friendly list.

#### **Example Usage:**
```python
text = "The government plays an important role."
result = cc.Analyze(text)
csv_row = cc.GetResultsArray(result)
print(csv_row)
```

Expected output:
```json
[6, 4.3, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
```

---

### **4Ô∏è‚É£ `ExportCaptures(filename, fileEncoding='utf-8-sig', wildcardsOnly=False, fullset=True)`**
Exports wildcard-captured words and their frequencies to a CSV file.

#### **Example Usage:**
```python
cc.ExportCaptures("captured_words.csv")
```

---

### **5Ô∏è‚É£ `ExportDict2007Format(dicOutFilename, fileEncoding, separateDicts=False, separateDictsFolder=None)`**
Exports the loaded dictionary in **LIWC-2007 format**.

#### **Example Usage:**
```python
cc.dict.ExportDict2007Format("dictionary_2007.dic")
```

---

### **6Ô∏è‚É£ `ExportDict2022Format(dicOutFilename, fileEncoding, **options)`**
Exports the loaded dictionary in **LIWC-22 format**.

#### **Example Usage:**
```python
cc.dict.ExportDict2022Format("dictionary_2022.dicx")
```

---

### **7Ô∏è‚É£ `ExportDictJSON(filename, fileEncoding, indent=4)`**
Exports the dictionary mapping to a JSON file.

#### **Example Usage:**
```python
cc.dict.ExportDictJSON("dictionary.json")
```

---

### **8Ô∏è‚É£ `UpdateCategories(dicTerm, newCategories)`**
Updates the categories associated with a dictionary term.

#### **Example Usage:**
```python
cc.dict.UpdateCategories(dicTerm="happiness", newCategories={"positive_emotion": 1.0, "joy": 0.5})
```

---

## üîÑ **Example: Processing a Large CSV File with `tqdm`**
This script reads a **large CSV file** and processes each text in the `"body"` column.

```python
import csv
from tqdm import tqdm
from contentcoder.ContentCoder import ContentCoder

cc = ContentCoder(dicFilename='dictionary.dic', fileEncoding='utf-8-sig')

with open("Comments.csv", "r", encoding="utf-8-sig") as csvfile:
    reader = csv.DictReader(csvfile)
    total_lines = sum(1 for _ in open("Comments.csv")) - 1  # Count rows

    for row in tqdm(reader, desc="Processing", unit=" comments"):
        text = row["body"]
        result = cc.Analyze(text)
```

---

## ‚ö° Performance Optimizations

- **Uses wildcard caching** to speed up regex evaluations.
- **Tokenization is optimized** for handling social media text.
- **Processes large datasets efficiently** using streaming CSV reads.

---

## üìú **Dictionary Formats Supported**
- **LIWC-2007 (`.dic`)**
- **LIWC-22 (`.dicx`, `.csv`)**
- **JSON Exports**
- **Custom Hierarchical Category Mapping**

---

## ü§ù **Contributing**
Pull requests are welcome! If you find bugs or have feature requests, open an issue.

---

## üìÑ **License**
MIT License ¬© 2021

---

## üìù **Acknowledgments**
Developed by **Ryan L. Boyd, Ph.D.**  
For academic and research purposes. Or, you know, whatever.
